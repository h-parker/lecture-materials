{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "**Aim**: SWBAT create and interpret a multiple linear regression model, explain why scaling is necessary in model interpretation, and explain multicolinearity is and how to avoid it. Also, SWBAT check the \n",
    "\n",
    "Breaking down today's lecture:\n",
    "1. What's the point?*\n",
    "2. How does it work?\n",
    "3. Interpretation: Why scaling is important\n",
    "4. Muliticolini-what? \n",
    "5. Linear Regression Assumptions*\n",
    "\n",
    "*Sections with stars also apply to simple linear regression*\n",
    "\n",
    "*Notebook based on Flatiron DS Immersive instructor Sean Abu's Multiple Linear Regression lecture.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# What's the point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You are a data scientist for the MTA. For your first project, they want you to predict the number of subway riders for each day. You decide to do a linear regression model predict the riders but need to gather data first. With a partner brainstorm a list of different variables you think would explain the number of daily riders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Why multiple linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Using all of the features you mentioned is _far_ more effective at predicting and understanding ridership!\n",
    "\n",
    "But, generally speaking, there are 3 major uses for (multiple) linear regression analysis.  \n",
    "\n",
    "1. Useful to **identify the strength of the effect** that the independent variables have on a dependent variable.\n",
    "    - Strong or weak relationship? \n",
    "\n",
    "\n",
    "2. **Quantitatively forecast effects or impacts of changes.**  That is, multiple linear regression analysis helps us to understand how much will the dependent variable change when we change the independent variables.  \n",
    "     - Multiple linear regression allows us to do so _comparatively_ -- does one feature have _more_ of an impact than another? \n",
    "     - Allows us the answer the question: What is the most important feature when predicting our target? \n",
    "\n",
    "\n",
    "3. **Predicts trends and future values.**  The multiple linear regression analysis can be used to get point estimates.  An example question may be “what will the price of gold be 6 month from now?”\n",
    "    - But, remember our cautions from last lecture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Code\n",
    "Basically the same as what we've already learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#read in car data\n",
    "df = sns.load_dataset('mpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year origin                       name  \n",
       "0          70    usa  chevrolet chevelle malibu  \n",
       "1          70    usa          buick skylark 320  \n",
       "2          70    usa         plymouth satellite  \n",
       "3          70    usa              amc rebel sst  \n",
       "4          70    usa                ford torino  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# building a linear regression model using statsmodel \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "lr_model = ols(formula='mpg~weight+horsepower+displacement+cylinders+acceleration', data=df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>mpg</td>       <th>  R-squared:         </th> <td>   0.708</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.704</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   186.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 16 Jul 2020</td> <th>  Prob (F-statistic):</th> <td>9.82e-101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:41:54</td>     <th>  Log-Likelihood:    </th> <td> -1120.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   2252.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   386</td>      <th>  BIC:               </th> <td>   2276.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>    <td>   46.2643</td> <td>    2.669</td> <td>   17.331</td> <td> 0.000</td> <td>   41.016</td> <td>   51.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weight</th>       <td>   -0.0052</td> <td>    0.001</td> <td>   -6.351</td> <td> 0.000</td> <td>   -0.007</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>horsepower</th>   <td>   -0.0453</td> <td>    0.017</td> <td>   -2.716</td> <td> 0.007</td> <td>   -0.078</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>displacement</th> <td>-8.313e-05</td> <td>    0.009</td> <td>   -0.009</td> <td> 0.993</td> <td>   -0.018</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cylinders</th>    <td>   -0.3979</td> <td>    0.411</td> <td>   -0.969</td> <td> 0.333</td> <td>   -1.205</td> <td>    0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>acceleration</th> <td>   -0.0291</td> <td>    0.126</td> <td>   -0.231</td> <td> 0.817</td> <td>   -0.276</td> <td>    0.218</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>38.561</td> <th>  Durbin-Watson:     </th> <td>   0.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  52.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.706</td> <th>  Prob(JB):          </th> <td>3.53e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.111</td> <th>  Cond. No.          </th> <td>3.87e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.87e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    mpg   R-squared:                       0.708\n",
       "Model:                            OLS   Adj. R-squared:                  0.704\n",
       "Method:                 Least Squares   F-statistic:                     186.9\n",
       "Date:                Thu, 16 Jul 2020   Prob (F-statistic):          9.82e-101\n",
       "Time:                        14:41:54   Log-Likelihood:                -1120.1\n",
       "No. Observations:                 392   AIC:                             2252.\n",
       "Df Residuals:                     386   BIC:                             2276.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "Intercept       46.2643      2.669     17.331      0.000      41.016      51.513\n",
       "weight          -0.0052      0.001     -6.351      0.000      -0.007      -0.004\n",
       "horsepower      -0.0453      0.017     -2.716      0.007      -0.078      -0.012\n",
       "displacement -8.313e-05      0.009     -0.009      0.993      -0.018       0.018\n",
       "cylinders       -0.3979      0.411     -0.969      0.333      -1.205       0.409\n",
       "acceleration    -0.0291      0.126     -0.231      0.817      -0.276       0.218\n",
       "==============================================================================\n",
       "Omnibus:                       38.561   Durbin-Watson:                   0.865\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               52.737\n",
       "Skew:                           0.706   Prob(JB):                     3.53e-12\n",
       "Kurtosis:                       4.111   Cond. No.                     3.87e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.87e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Multiple linear regression has the form:\n",
    "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + + \\beta_3 X_3\\cdots + \\beta_k X_k + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Your turn #1\n",
    "Based on what we learned about simple linear regression what do each of the following represent?\n",
    "- $Y$\n",
    "- $\\beta_0$\n",
    "- $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Answer:\n",
    "- $Y$ - predicted values\n",
    "- $\\beta_0$ - y intercept\n",
    "- $\\epsilon$ - error term (random variation)\n",
    "\n",
    "This leads us into interpreting the rest of those coefficients! \n",
    "\n",
    "The good news is, we're 90% of the way to understanding multiple linear regression 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### What are $\\beta_1 ... \\beta_n$? How are they computed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "They are our coefficients of the features in the model, just like in simple linear regression. Each of coefficients represent the impact of $X_i$ on $Y$, *all other features held constant*. \n",
    "- If no other features changed, what impact would increasing $X_i$ by one unit have on the other features? \n",
    "\n",
    "\n",
    "How are they computed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Multiple Regression: The Coefficient of Determination\n",
    "Multiple linear regression is simply a linear regression with more than one predictor, or independent variables. \n",
    "\n",
    "Recall the interpretation of $R^2$ in simple linear regression:\n",
    "- $R^2$ represents the proportion of variance explained by the model. \n",
    "\n",
    "**So, we have another interpretation of Multiple Linear regression:**\n",
    "\n",
    "By including more predictors, we make the model more complex in an effort to account for more variance in our target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Interpretation of the Model Parameters\n",
    "- Each β parameter represents the change in the mean response, E(y), per unit increase in the associated predictor variable when all the other predictors are held constant.\n",
    "- For example, β1 represents the estimated change in the mean response, E(y), per unit increase in x1 when x2, x3, ..., xp−1 are held constant.\n",
    "- The intercept term, β0, represents the estimated mean response, E(y), when all the predictors x1, x2, ..., xp−1, are all zero (which may or may not have any practical meaning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Multicollinearity \n",
    "\n",
    "**Multicollinearity** occurs when independent variables in a regression model are very highly correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n",
    "\n",
    "The interpretation of a regression coefficient is that it represents the mean change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "There are two basic kinds of multicollinearity:\n",
    "\n",
    "- **Structural multicollinearity:** This type occurs when we create a model term using other terms. In other words, it’s a byproduct of the model that we specify rather than being present in the data itself. For example, if you square term X to model curvature, clearly there is a correlation between X and X2.\n",
    "- **Data multicollinearity:** This type of multicollinearity is present in the data itself rather than being an artifact of our model. Observational experiments are more likely to exhibit this kind of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### What Problems Do Multicollinearity Cause?\n",
    "\n",
    "Multicollinearity causes the following two basic types of problems:\n",
    "\n",
    "- The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model.\n",
    "- Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Do I Have to Fix Multicollinearity?\n",
    "\n",
    "The need to reduce multicollinearity depends on its severity and your primary goal for your regression model. Keep the following three points in mind:\n",
    "\n",
    "- The severity of the problems increases with the degree of the multicollinearity. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "- Multicollinearity affects only the specific independent variables that are correlated. Therefore, if multicollinearity is not present for the independent variables that you are particularly interested in, you may not need to resolve it. \n",
    "- Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "***That being said, the easies way to deal with multicollinearity is just to remove one of the variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.matshow(df.corr())\n",
    "plt.xticks(range(len(df.columns)), df.columns)\n",
    "plt.yticks(range(len(df.columns)), df.columns)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create a Better Looking Heatmap with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def CorrMtx(df, dropDuplicates = True):\n",
    "\n",
    "    # Your dataset is already a correlation matrix.\n",
    "    # If you have a dateset where you need to include the calculation\n",
    "    # of a correlation matrix, just uncomment the line below:\n",
    "    # df = df.corr()\n",
    "\n",
    "    # Exclude duplicate correlations by masking uper right values\n",
    "    if dropDuplicates:    \n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set background color / chart style\n",
    "    sns.set_style(style = 'white')\n",
    "\n",
    "    # Set up  matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Add diverging colormap from red to blue\n",
    "    cmap = sns.diverging_palette(250, 10, as_cmap=True)\n",
    "\n",
    "    # Draw correlation plot with or without duplicates\n",
    "    if dropDuplicates:\n",
    "        sns.heatmap(df, mask=mask, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "    else:\n",
    "        sns.heatmap(df, cmap=cmap, \n",
    "                square=True,\n",
    "                linewidth=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "\n",
    "CorrMtx(corr, dropDuplicates = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Even more examples to make your correlation heatmap look good\n",
    "https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Rerun the Model After Removing the highly correlate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mlr_model = ols(formula='mpg~____', data=____).fit()\n",
    "mlr_model.____()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Handling Categorical Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df['origin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For the column of `origin`, we can see that the values come through as strings that represent a category.  We can not put a string through as a value for a linear model. Instead we use dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A **dummy variable** (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc.\n",
    "\n",
    "Technically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values. As a practical matter, regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df['origin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By creating these dummy variables. We can now include them in the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dummy_model = ols(formula='mpg~weight+horsepower+cylinders+acceleration+origin_europe+origin_japan', data=dummy_df).fit()\n",
    "dummy_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "You might have noticed that I did not include the `origin_usa` column in this model.\n",
    "\n",
    "That dummy variable would redundant; it carries no new information. And it creates a severe multicollinearity problem for the analysis. \n",
    "\n",
    "If we know the value of the `origin_europe` and the `origin'japan` columns, then we also know the value of the `origin_usa` column.\n",
    "\n",
    "Using all the dummy variables off a category, is known as the dummy variable trap. Avoid this trap!\n",
    "\n",
    "There are tow ways to handle this. One is as seen above where I just don't include one of the variables in the model. Another is to automatically drop one of the columns when I use `pd.get_dummies`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['origin'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Interpreting the coefficients of Dummy Variables\n",
    "\n",
    "So how do we interpret the coefficient of a dummy variable? Looking at our model output from above. If a car originated in Japan, it will have a value of 1 for the variable `origin_japan`. So we would multiply the value of 1 by the coefficient for that variable and add that to our final MPG prediction.  \n",
    "\n",
    "If the car orginated in the USA it would have a value of zero for both the `origin_japan` and `origin_europe` columns. Therefore, the coefficients for those variables who not impact the final prediction.  \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**On average what is the difference in the gas mileage for cars from Japan verus cars from USA in this dataset?**\n",
    "\n",
    "**On average what is the difference in the gas mileage for cars from Japan verus cars from Europe in this dataset?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "https://towardsdatascience.com/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Scaling Data\n",
    "\n",
    "\n",
    "Most of the times, your dataset will contain features highly varying in magnitudes, units and range. For linear regression models, this makes it difficult to compare the sizes of the coefficients for different variables. Also, many machine learning algorithms use Eucledian distance between two data points in their computations, and data of different scale, distorts those distances.\n",
    "\n",
    "Three common ways to scale the data are:\n",
    "1. **Standardization**: This redistributes the features with their mean μ = 0 and standard deviation σ =1 . sklearn.preprocessing.scale helps us implementing standardisation in python. \n",
    "$$x' =\\frac{x-\\bar{x}}{\\sigma}$$\n",
    "\n",
    "2. **Mean Normalization**: This distribution will have values between -1 and 1 with μ=0.\n",
    "\n",
    "$$x' =\\frac{x-\\bar{x}}{max(x)- min(x)}$$\n",
    "\n",
    "3. **Min-Max Scaling**: This scaling brings the value between 0 and 1.\n",
    "\n",
    "$$x' =\\frac{x-min(x)}{max(x)- min(x)}$$\n",
    "\n",
    "\n",
    "\n",
    "https://medium.com/@swethalakshmanan14/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler.fit_transform(dummy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dummy_df.drop(['name', 'origin_usa'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scaler.fit_transform(dummy_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "Another way to use the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dummy_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "subset= ['cylinders', 'horsepower', 'weight',\n",
    "       'acceleration', 'origin_europe', 'origin_japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "subset_scaled = []\n",
    "for var in subset:\n",
    "    new_col = var +\"_scaled\"\n",
    "    dummy_df[new_col] = scaler.fit_transform(dummy_df[[var]])\n",
    "    subset_scaled.append(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dummy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "scaled_model = ols(formula='mpg~weight_scaled+horsepower_scaled+cylinders_scaled+acceleration_scaled+origin_europe_scaled+origin_japan_scaled', data=dummy_df).fit()\n",
    "scaled_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now our coefficients are on a similiar scale which allows us to compare the size of the coeffiecents to make some inferences about which features have a bigger impact on the MPG of a car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Resources\n",
    "\n",
    "Everything about regression:  https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-tutorial-and-examples\n",
    "\n",
    "Statsmodels example: https://datatofish.com/statsmodels-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
